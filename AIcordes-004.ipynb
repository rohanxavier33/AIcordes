{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chord Recognition with Deep Learning\n",
    "## Using the McGill Billboard Dataset\n",
    "\n",
    "Dataset: https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Dropout, Conv1D, MaxPooling1D, \n",
    "    BatchNormalization, Activation, Reshape, add, multiply,\n",
    "    Bidirectional, Lambda, Concatenate\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is detected\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 5  # Number of epochs for training\n",
    "SAMPLE_SIZE = 150    # Number of songs to use in the sample\n",
    "BATCH_SIZE = 256    # Batch size for training\n",
    "RANDOM_STATE = 33\n",
    "HYPERPARAMETER_TRIALS = 5\n",
    "TRIAL_EPOCHS = 5\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "    \n",
    "# Path configuration\n",
    "data_dir = Path('data/McGill-Billboard')\n",
    "chordino_dir = data_dir / 'chordino'\n",
    "lab_dir = data_dir / 'lab'\n",
    "annotations_dir = data_dir / 'annotations'\n",
    "index_path = data_dir / 'index.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset index\n",
    "index_df = pd.read_csv(index_path)\n",
    "print(f\"Total entries in index: {len(index_df)}\")\n",
    "print(f\"Entries with complete data: {index_df['title'].notna().sum()}\")\n",
    "\n",
    "# Clean dataset\n",
    "index_df = index_df.dropna(subset=['title'])\n",
    "print(f'\\nTotal Entries after cleaning: {len(index_df)}')\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Unique songs: {index_df['title'].nunique()}\")\n",
    "print(f\"Unique artists: {index_df['artist'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis\n",
    "index_df['chart_date'] = pd.to_datetime(index_df['chart_date'])\n",
    "index_df['year'] = index_df['chart_date'].dt.year\n",
    "index_df['decade'] = (index_df['year'] // 10) * 10\n",
    "\n",
    "decade_counts = index_df['decade'].value_counts().sort_index()\n",
    "print(\"\\nSongs per decade:\")\n",
    "print(decade_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data directories\n",
    "song_ids = [d.name for d in chordino_dir.iterdir() if d.is_dir()]\n",
    "lab_ids = [d.name for d in lab_dir.iterdir() if d.is_dir()]\n",
    "common_ids = set(song_ids).intersection(set(lab_ids))\n",
    "\n",
    "print(f\"\\nChroma features available for: {len(song_ids)} songs\")\n",
    "print(f\"Lab files available for: {len(lab_ids)} songs\")\n",
    "print(f\"Common entries: {len(common_ids)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Chord Distribution Across the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chord_distribution(song_ids, top_n=20):\n",
    "    \"\"\"Get the distribution of chords across all songs\"\"\"\n",
    "    all_chords = Counter()\n",
    "    processed_songs = 0\n",
    "    \n",
    "    # Limit to sample size if needed\n",
    "    # sample_ids = song_ids[:min(SAMPLE_SIZE if SAMPLE_SIZE is not None else len(song_ids), len(song_ids))]\n",
    "    sample_ids = song_ids\n",
    "\n",
    "    for song_id in tqdm(sample_ids, desc=\"Analyzing chord distribution\"):\n",
    "        lab_path = lab_dir / song_id / \"full.lab\"\n",
    "        \n",
    "        if lab_path.exists():\n",
    "            # Load and count chord labels\n",
    "            lab_data = pd.read_csv(lab_path, sep='\\t', header=None, names=['start_time', 'end_time', 'chord'])\n",
    "            chord_counts = lab_data['chord'].value_counts().to_dict()\n",
    "            \n",
    "            # Add to overall counter\n",
    "            all_chords.update(chord_counts)\n",
    "            processed_songs += 1\n",
    "    \n",
    "    print(f\"Processed {processed_songs} songs for chord distribution\")\n",
    "    return all_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chord distribution across the sample\n",
    "chord_distribution = get_chord_distribution(list(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top chords\n",
    "num_top_chords = 300\n",
    "top_chords = chord_distribution.most_common(num_top_chords)\n",
    "\n",
    "# Print formatted table\n",
    "print(f\"Most Common Chords Across the Dataset (Top {num_top_chords})\")\n",
    "print(\"-\" * 36)\n",
    "print(f\"{'Chord':<15} {'Frequency':>15}\")\n",
    "print(\"-\" * 36)\n",
    "for chord, count in top_chords:\n",
    "    print(f\"{chord:<15} {count:>15,}\")\n",
    "print(\"-\" * 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top chords\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_chords = pd.Series(dict(chord_distribution.most_common(30)))\n",
    "top_chords.plot(kind='bar', color='lightseagreen')\n",
    "plt.title('Most Common Chords Across the Dataset (Top 30)')\n",
    "plt.xlabel('Chord')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "total_chords = sum(chord_distribution.values())\n",
    "unique_chords = len(chord_distribution)\n",
    "print(f\"Total chord occurrences: {total_chords}\")\n",
    "print(f\"Number of unique chords: {unique_chords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much of the dataset is covered by the top N chords?\n",
    "def coverage_analysis(chord_distribution, tops=[10, 20, 50, 100, 200]):\n",
    "    total = sum(chord_distribution.values())\n",
    "    for n in tops:\n",
    "        top_n = sum(dict(chord_distribution.most_common(n)).values())\n",
    "        print(f\"Top {n} chords cover {top_n/total*100:.2f}% of all chord occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_analysis(chord_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chord Simplification (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_chord(chord):\n",
    "    \"\"\"\n",
    "    Simplify chord to one of the following types:\n",
    "    - major\n",
    "    - minor\n",
    "    - m7b5 (half-diminished)\n",
    "    - power chords (5)\n",
    "    - dominant 7\n",
    "    - sus2\n",
    "    - sus4\n",
    "    - aug (augmented)\n",
    "    - dim (diminished)\n",
    "    - For 1/1 chords, just output the root\n",
    "    \"\"\"\n",
    "    if chord == 'N' or chord == 'X':\n",
    "        return chord\n",
    "    \n",
    "    # Extract root and quality\n",
    "    parts = chord.split(':')\n",
    "    if len(parts) < 2:\n",
    "        return chord  # Can't parse\n",
    "    \n",
    "    root = parts[0]\n",
    "    quality = parts[1]\n",
    "    \n",
    "    # Handle 1/1 chords (just return the root)\n",
    "    if quality == '1/1':\n",
    "        return f\"{root}\"\n",
    "    \n",
    "    # Simple chord types that we keep as is\n",
    "    if quality in ['maj', 'min', '7', 'sus2', 'sus4', 'aug', 'dim', '5']:\n",
    "        return chord\n",
    "    \n",
    "    # Handle half-diminished\n",
    "    if quality == 'hdim7':\n",
    "        return f\"{root}:m7b5\"\n",
    "    \n",
    "    # Simplify other chord types\n",
    "    if 'min' in quality:\n",
    "        return f\"{root}:min\"\n",
    "    \n",
    "    if 'maj' in quality and '7' not in quality:\n",
    "        return f\"{root}:maj\"\n",
    "    \n",
    "    if 'maj7' in quality or 'maj9' in quality or 'maj13' in quality:\n",
    "        return f\"{root}:maj\"\n",
    "    \n",
    "    if '7' in quality or '9' in quality or '13' in quality:\n",
    "        return f\"{root}:7\"\n",
    "    \n",
    "    if '5' in quality:\n",
    "        return f\"{root}:5\"\n",
    "    \n",
    "    if 'sus' in quality:\n",
    "        if 'sus2' in quality:\n",
    "            return f\"{root}:sus2\"\n",
    "        if 'sus4' in quality:\n",
    "            return f\"{root}:sus4\"\n",
    "    \n",
    "    if 'aug' in quality:\n",
    "        return f\"{root}:aug\"\n",
    "    \n",
    "    if 'dim' in quality:\n",
    "        return f\"{root}:dim\"\n",
    "    \n",
    "    # Default to major if we can't determine\n",
    "    return f\"{root}:maj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze the impact of simplification\n",
    "def analyze_chord_simplification(chord_distribution):\n",
    "    total_chords = sum(chord_distribution.values())\n",
    "    original_unique = len(chord_distribution)\n",
    "    \n",
    "    # Create a new distribution with simplified chords\n",
    "    simplified_distribution = Counter()\n",
    "    mapping = {}\n",
    "    \n",
    "    for chord, count in chord_distribution.items():\n",
    "        simplified = simplify_chord(chord)\n",
    "        simplified_distribution[simplified] += count\n",
    "        mapping[chord] = simplified\n",
    "    \n",
    "    # Count how many chords were changed\n",
    "    changed_chords = sum(1 for original, simplified in mapping.items() \n",
    "                          if original != simplified)\n",
    "    \n",
    "    simplified_unique = len(simplified_distribution)\n",
    "    \n",
    "    print(f\"Original number of unique chords: {original_unique}\")\n",
    "    print(f\"Simplified number of unique chords: {simplified_unique}\")\n",
    "    print(f\"Reduction in unique chords: {original_unique - simplified_unique} ({(original_unique - simplified_unique) / original_unique * 100:.2f}%)\")\n",
    "    print(f\"Number of chord types that were changed: {changed_chords} ({changed_chords / original_unique * 100:.2f}%)\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    print(\"\\nCoverage analysis after simplification:\")\n",
    "    total = sum(simplified_distribution.values())\n",
    "    for n in [10, 20, 50]:\n",
    "        top_n = sum(dict(simplified_distribution.most_common(n)).values())\n",
    "        print(f\"Top {n} simplified chords cover {top_n/total*100:.2f}% of all chord occurrences\")\n",
    "    \n",
    "    # Most common simplified chords\n",
    "    print(\"\\nMost common simplified chords:\")\n",
    "    for chord, count in simplified_distribution.most_common(20):\n",
    "        print(f\"{chord:15} {count}\")\n",
    "    \n",
    "    return simplified_distribution, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "simplified_distribution, chord_mapping = analyze_chord_simplification(chord_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the simplified chord distribution\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_simplified_chords = pd.Series(dict(simplified_distribution.most_common(20)))\n",
    "top_simplified_chords.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Most Common Simplified Chords (Top 20)')\n",
    "plt.xlabel('Chord')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which original chords were mapped to each simplified chord\n",
    "def show_mapping_details(simplified_chord, chord_mapping, chord_distribution, top_n=10):\n",
    "    original_chords = [(orig, chord_distribution[orig]) for orig, simp in chord_mapping.items() \n",
    "                      if simp == simplified_chord]\n",
    "    sorted_originals = sorted(original_chords, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nOriginal chords that were simplified to {simplified_chord}:\")\n",
    "    for orig, count in sorted_originals[:top_n]:\n",
    "        print(f\"{orig:20} {count}\")\n",
    "    if len(sorted_originals) > top_n:\n",
    "        print(f\"...and {len(sorted_originals) - top_n} more\")\n",
    "\n",
    "# Display mapping details for a few important simplified chords\n",
    "for chord in simplified_distribution.most_common(5):\n",
    "    show_mapping_details(chord[0], chord_mapping, chord_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the common_ids based on SAMPLE_SIZE\n",
    "sampled_ids = random.sample(list(common_ids), min(SAMPLE_SIZE, len(common_ids)))\n",
    "print(f\"\\nUsing {len(sampled_ids)} songs for analysis (SAMPLE_SIZE={SAMPLE_SIZE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Song Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random example analysis - select from our sampled IDs for consistency\n",
    "example_id = random.choice(sampled_ids)\n",
    "\n",
    "# Load chroma features\n",
    "chroma_path = chordino_dir / example_id / 'bothchroma.csv'\n",
    "chroma_data = pd.read_csv(chroma_path, header=None)\n",
    "print(f\"\\nChroma data shape for {example_id}: {chroma_data.shape}\")\n",
    "\n",
    "# Load chord labels\n",
    "lab_data = pd.read_csv(lab_dir / example_id / \"full.lab\", sep='\\t', names=['start', 'end', 'chord'])\n",
    "\n",
    "# Apply chord simplification to the loaded data\n",
    "lab_data['original_chord'] = lab_data['chord']\n",
    "lab_data['chord'] = lab_data['chord'].apply(simplify_chord)\n",
    "\n",
    "print(f\"\\nChord segments count: {len(lab_data)}\")\n",
    "print(\"\\nMost common simplified chords:\")\n",
    "print(lab_data['chord'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original chords\n",
    "print(\"\\nComparison with original chords:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': lab_data['original_chord'].value_counts().head(60),\n",
    "    'Simplified': lab_data['chord'].value_counts().head(60)\n",
    "})\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot the chroma features\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(chroma_data.iloc[:, 2:14].T, cmap='coolwarm', cbar=True)\n",
    "plt.title(f'Chroma Features for Song {example_id}')\n",
    "plt.xlabel('Time frames')\n",
    "plt.ylabel('Pitch Class')\n",
    "\n",
    "# Plot chord distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "lab_data['chord'].value_counts().head(10).plot(kind='barh', color='lightgreen')\n",
    "plt.title('Top 10 Simplified Chords')\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_song_data(song_id, chordino_dir, lab_dir, simplify_chord_fn):\n",
    "    \"\"\"\n",
    "    Load and align chroma features with chord labels for a single song\n",
    "    \n",
    "    Args:\n",
    "        song_id: ID of the song\n",
    "        chordino_dir: Directory containing chroma features\n",
    "        lab_dir: Directory containing chord labels\n",
    "        simplify_chord_fn: Function to simplify chord labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (chroma_features, aligned_chord_labels)\n",
    "    \"\"\"\n",
    "    # Load chroma features\n",
    "    chroma_path = chordino_dir / song_id / 'bothchroma.csv'\n",
    "    chroma_data = pd.read_csv(chroma_path, header=None)\n",
    "    chroma_array = chroma_data.iloc[:, 2:14].values.astype(np.float32)\n",
    "    chroma_array = np.nan_to_num(chroma_array)  # Handle NaN values\n",
    "    \n",
    "    # Load chord labels\n",
    "    lab_path = lab_dir / song_id / \"full.lab\"\n",
    "    chord_data = pd.read_csv(lab_path, sep='\\t', names=['start_time', 'end_time', 'chord'])\n",
    "    \n",
    "    # Apply chord simplification\n",
    "    chord_data['chord'] = chord_data['chord'].apply(simplify_chord_fn)\n",
    "    \n",
    "    # Align chroma features with chord labels\n",
    "    hop_size = 0.01  # 10ms hop size (typical for Chordino)\n",
    "    chroma_times = np.arange(len(chroma_array)) * hop_size\n",
    "    \n",
    "    aligned_chords = []\n",
    "    for t in chroma_times:\n",
    "        matching = chord_data[(chord_data['start_time'] <= t) & (chord_data['end_time'] > t)]\n",
    "        if not matching.empty:\n",
    "            aligned_chords.append(matching['chord'].values[0])\n",
    "        else:\n",
    "            aligned_chords.append('N')  # No chord label\n",
    "    \n",
    "    return chroma_array, np.array(aligned_chords)\n",
    "\n",
    "def create_windowed_examples(features, labels, window_size=15, hop_size=1):\n",
    "    \"\"\"\n",
    "    Create windowed examples from a sequence of features and labels\n",
    "    \n",
    "    Args:\n",
    "        features: Chroma features array (frames × features)\n",
    "        labels: Chord labels array\n",
    "        window_size: Number of frames in each window\n",
    "        hop_size: Number of frames to hop between windows\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (windowed_features, window_labels)\n",
    "    \"\"\"\n",
    "    num_frames = len(features)\n",
    "    X_windows, y_labels = [], []\n",
    "    \n",
    "    for i in range(0, num_frames - window_size + 1, hop_size):\n",
    "        # Extract window of chroma features\n",
    "        window = features[i:i + window_size]\n",
    "        \n",
    "        # Use the chord label from the center of the window\n",
    "        center_idx = i + window_size // 2\n",
    "        label = labels[center_idx]\n",
    "        \n",
    "        X_windows.append(window)\n",
    "        y_labels.append(label)\n",
    "    \n",
    "    return np.array(X_windows), np.array(y_labels)\n",
    "\n",
    "def prepare_dataset(song_ids, chordino_dir, lab_dir, simplify_chord_fn, \n",
    "                   window_size=30, hop_size=1, test_size=0.2, val_size=0.1,\n",
    "                   random_state=42, max_songs=None):\n",
    "    \"\"\"\n",
    "    Prepare complete dataset with windows, including train/val/test splits\n",
    "    \n",
    "    Args:\n",
    "        song_ids: List of song IDs\n",
    "        chordino_dir: Directory containing chroma features\n",
    "        lab_dir: Directory containing chord labels\n",
    "        simplify_chord_fn: Function to simplify chord labels\n",
    "        window_size: Number of frames in each window\n",
    "        hop_size: Number of frames to hop between windows\n",
    "        test_size: Proportion of data for testing\n",
    "        val_size: Proportion of training data for validation\n",
    "        random_state: Random seed for reproducibility\n",
    "        max_songs: Maximum number of songs to process (for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dataset splits and metadata\n",
    "    \"\"\"\n",
    "    # Limit number of songs if needed\n",
    "    if max_songs is not None:\n",
    "        song_ids = song_ids[:min(max_songs, len(song_ids))]\n",
    "    \n",
    "    print(f\"Preparing dataset from {len(song_ids)} songs...\")\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    processed_songs = 0\n",
    "    \n",
    "    # Process each song\n",
    "    for song_id in tqdm(song_ids, desc=\"Processing songs\"):\n",
    "        try:\n",
    "            # Load and align features and labels\n",
    "            features, labels = load_and_align_song_data(\n",
    "                song_id, chordino_dir, lab_dir, simplify_chord_fn\n",
    "            )\n",
    "            \n",
    "            # Create windowed examples\n",
    "            X_windows, y_windows = create_windowed_examples(\n",
    "                features, labels, window_size, hop_size\n",
    "            )\n",
    "            \n",
    "            all_features.append(X_windows)\n",
    "            all_labels.append(y_windows)\n",
    "            processed_songs += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing song {song_id}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully processed {processed_songs} songs\")\n",
    "    \n",
    "    # Combine data from all songs\n",
    "    X = np.vstack(all_features)\n",
    "    y = np.concatenate(all_labels)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    \n",
    "    print(f\"Total examples: {len(X)}\")\n",
    "    print(f\"Number of chord classes: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    # Split into train+val and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y_onehot, test_size=test_size, random_state=random_state, stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    # Further split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size/(1-test_size), \n",
    "        random_state=random_state, stratify=np.argmax(y_train_val, axis=1)\n",
    "    )\n",
    "    \n",
    "    # Return dataset splits and metadata\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'label_encoder': label_encoder,\n",
    "        'window_size': window_size,\n",
    "        'hop_size': hop_size,\n",
    "        'num_features': X.shape[2],\n",
    "        'num_classes': y_onehot.shape[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(\n",
    "    song_ids=sampled_ids,\n",
    "    chordino_dir=chordino_dir,\n",
    "    lab_dir=lab_dir,\n",
    "    simplify_chord_fn=simplify_chord,\n",
    "    window_size=30,\n",
    "    hop_size=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_chord_model(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    window_size,\n",
    "    conv_filters=[64, 128],\n",
    "    conv_kernel_size=3,\n",
    "    pool_size=2,\n",
    "    use_bottleneck=True,\n",
    "    bottleneck_size=32,\n",
    "    use_residual=True,\n",
    "    use_attention=True,\n",
    "    lstm_units=[128, 64],\n",
    "    dense_units=[64],\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an optimized hybrid model for chord recognition with configurable hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Number of features (12 for chroma)\n",
    "        num_classes: Number of chord classes\n",
    "        window_size: Number of frames in each window\n",
    "        conv_filters: List of filter sizes for Conv1D layers\n",
    "        conv_kernel_size: Kernel size for Conv1D layers\n",
    "        pool_size: Pool size for MaxPooling1D layers\n",
    "        use_bottleneck: Whether to use bottleneck layers\n",
    "        bottleneck_size: Size of bottleneck layer\n",
    "        use_residual: Whether to use residual connections\n",
    "        use_attention: Whether to use attention mechanism\n",
    "        lstm_units: List of unit sizes for LSTM layers\n",
    "        dense_units: List of unit sizes for Dense layers\n",
    "        dropout_rate: Dropout rate\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    # Check if window_size is too small for the number of pooling layers\n",
    "    min_sequence_length = window_size\n",
    "    for _ in range(len(conv_filters)):\n",
    "        min_sequence_length = min_sequence_length // pool_size\n",
    "        if min_sequence_length < 1:\n",
    "            # Adjust the model to use fewer pooling layers\n",
    "            conv_filters = conv_filters[:1]  # Use only first conv layer\n",
    "            print(f\"Warning: Window size {window_size} is too small for multiple pooling layers. Reducing to {len(conv_filters)} conv layers.\")\n",
    "            \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(window_size, input_shape))\n",
    "    \n",
    "    # CNN feature extraction\n",
    "    x = inputs\n",
    "    skip_connection = None\n",
    "    \n",
    "    # Apply convolutional layers\n",
    "    for i, filters in enumerate(conv_filters):\n",
    "        x = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name=f'conv1d_{i+1}'\n",
    "        )(x)\n",
    "        x = BatchNormalization(name=f'bn_conv_{i+1}')(x)\n",
    "        \n",
    "        # Save tensor for residual connection\n",
    "        if i == 0 and use_residual:\n",
    "            skip_connection = x\n",
    "            \n",
    "        # Apply pooling with safety check\n",
    "        if i < len(conv_filters) - 1:  # No pooling after last conv layer\n",
    "            # Get current temporal dimension size\n",
    "            current_length = x.shape[1]\n",
    "            \n",
    "            # Only apply pooling if we have enough temporal dimension left\n",
    "            if current_length >= pool_size:\n",
    "                x = MaxPooling1D(pool_size=pool_size, name=f'pool_{i+1}')(x)\n",
    "            else:\n",
    "                print(f\"Skipping pooling at layer {i+1} because sequence length {current_length} < pool_size {pool_size}\")\n",
    "    \n",
    "    # Apply bottleneck if specified\n",
    "    if use_bottleneck:\n",
    "        x = Conv1D(\n",
    "            filters=bottleneck_size,\n",
    "            kernel_size=1,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name='bottleneck'\n",
    "        )(x)\n",
    "        x = BatchNormalization(name='bn_bottleneck')(x)\n",
    "    \n",
    "    # Add residual connection if specified\n",
    "    if use_residual and skip_connection is not None:\n",
    "        # Skip residual connection if temporal dimension is zero\n",
    "        if x.shape[1] <= 0 or skip_connection.shape[1] <= 0:\n",
    "            print(\"Skipping residual connection due to invalid dimensions\")\n",
    "        else:\n",
    "            # Ensure compatible shapes\n",
    "            if skip_connection.shape[1:] != x.shape[1:]:\n",
    "                skip_connection = Conv1D(\n",
    "                    filters=x.shape[-1],\n",
    "                    kernel_size=1,\n",
    "                    padding='same',\n",
    "                    name='skip_adjust'\n",
    "                )(skip_connection)\n",
    "                \n",
    "                # If shapes still don't match in time dimension and both dimensions are valid\n",
    "                if skip_connection.shape[1] != x.shape[1] and skip_connection.shape[1] > 0 and x.shape[1] > 0:\n",
    "                    # Safely calculate pool factor\n",
    "                    if x.shape[1] == 0:\n",
    "                        # Skip residual if target dimension is 0\n",
    "                        print(\"Cannot create residual connection - target dimension is 0\")\n",
    "                    else:\n",
    "                        pool_factor = int(skip_connection.shape[1] / x.shape[1])\n",
    "                        if pool_factor > 1:\n",
    "                            skip_connection = MaxPooling1D(\n",
    "                                pool_size=pool_factor,\n",
    "                                name='skip_pool'\n",
    "                            )(skip_connection)\n",
    "            \n",
    "            # Only add the residual connection if dimensions match now\n",
    "            if skip_connection.shape[1:] == x.shape[1:]:\n",
    "                x = add([x, skip_connection], name='residual_connection')\n",
    "            else:\n",
    "                print(f\"Skipping residual connection. Shapes don't match: {skip_connection.shape} vs {x.shape}\")\n",
    "    \n",
    "    # Recurrent layers\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_sequences = i < len(lstm_units) - 1\n",
    "        \n",
    "        # Use bidirectional LSTMs\n",
    "        x = Bidirectional(\n",
    "            LSTM(units, return_sequences=return_sequences),\n",
    "            name=f'bilstm_{i+1}'\n",
    "        )(x)\n",
    "        \n",
    "        # Only apply dropout to intermediate layers\n",
    "        if i < len(lstm_units) - 1:\n",
    "            x = Dropout(dropout_rate, name=f'lstm_dropout_{i+1}')(x)\n",
    "    \n",
    "    # Apply attention mechanism if specified\n",
    "    if use_attention and len(lstm_units) > 0:\n",
    "        # If last LSTM doesn't return sequences, we need another LSTM for attention\n",
    "        if not return_sequences:\n",
    "            x = Reshape((1, x.shape[-1]))(x)\n",
    "            return_sequences = True\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        attention = Dense(1, activation='tanh', name='attention_dense')(x)\n",
    "        attention = Reshape((attention.shape[1],), name='attention_reshape')(attention)\n",
    "        attention = Activation('softmax', name='attention_weights')(attention)\n",
    "        attention = Reshape((attention.shape[1], 1), name='attention_reshape2')(attention)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        x = multiply([x, attention], name='apply_attention')\n",
    "        x = Lambda(lambda x: K.sum(x, axis=1), name='context_vector')(x)\n",
    "    \n",
    "    # Dense classification layers\n",
    "    for i, units in enumerate(dense_units):\n",
    "        x = Dense(units, activation='relu', name=f'dense_{i+1}')(x)\n",
    "        x = BatchNormalization(name=f'bn_dense_{i+1}')(x)\n",
    "        x = Dropout(dropout_rate, name=f'dense_dropout_{i+1}')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_callbacks(\n",
    "    checkpoint_path='best_chord_model.keras',\n",
    "    patience_early=15,\n",
    "    patience_lr=7,\n",
    "    min_delta=0.001,\n",
    "    factor=0.5,\n",
    "    min_lr=0.00001,\n",
    "    monitor='val_loss'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create callbacks for model training\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to save best model\n",
    "        patience_early: Patience for early stopping\n",
    "        patience_lr: Patience for learning rate reduction\n",
    "        min_delta: Minimum change to count as improvement\n",
    "        factor: Factor by which to reduce learning rate\n",
    "        min_lr: Minimum learning rate\n",
    "        monitor: Metric to monitor\n",
    "    \n",
    "    Returns:\n",
    "        List of callbacks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create logs directory\n",
    "    log_dir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        # Early stopping to prevent overfitting\n",
    "        EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            min_delta=min_delta,\n",
    "            patience=patience_early,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        \n",
    "        # Save best model\n",
    "        ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=monitor,\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate when plateau\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=monitor,\n",
    "            factor=factor,\n",
    "            patience=patience_lr,\n",
    "            verbose=1,\n",
    "            min_lr=min_lr\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_optimized_chord_model(\n",
    "    input_shape=dataset['num_features'],\n",
    "    num_classes=dataset['num_classes'],\n",
    "    window_size=dataset['window_size']\n",
    ")\n",
    "callbacks = create_model_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "def hyperparameter_tuning(dataset, n_trials=5, epochs=15):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for the chord recognition model.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dictionary containing dataset splits and metadata\n",
    "        n_trials: Number of hyperparameter combinations to try\n",
    "        epochs: Number of epochs to train each model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Best hyperparameters found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    hyperparameter_space = {\n",
    "        'conv_filters': [\n",
    "            [32, 64],\n",
    "            [64, 128], \n",
    "            [64, 128, 256],\n",
    "            [32, 64, 128, 256]\n",
    "        ],\n",
    "        'conv_kernel_size': [3, 5, 7],\n",
    "        'pool_size': [2, 3],\n",
    "        'use_bottleneck': [True, False],\n",
    "        'bottleneck_size': [16, 32, 64],\n",
    "        'use_residual': [True, False],\n",
    "        'use_attention': [True, False],\n",
    "        'lstm_units': [\n",
    "            [64],\n",
    "            [128, 64],\n",
    "            [256, 128]\n",
    "        ],\n",
    "        'dense_units': [\n",
    "            [64],\n",
    "            [128, 64],\n",
    "            [256, 128, 64]\n",
    "        ],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "        'learning_rate': [0.01, 0.001, 0.0005, 0.0001]\n",
    "    }\n",
    "    \n",
    "    # Track best performance and parameters\n",
    "    best_val_acc = 0\n",
    "    best_params = None\n",
    "    trials_results = []\n",
    "    \n",
    "    print(f\"Starting hyperparameter tuning with {n_trials} trials...\")\n",
    "    for trial in range(n_trials):\n",
    "        # Sample hyperparameters\n",
    "        params = {\n",
    "            'conv_filters': random.choice(hyperparameter_space['conv_filters']),\n",
    "            'conv_kernel_size': random.choice(hyperparameter_space['conv_kernel_size']),\n",
    "            'pool_size': random.choice(hyperparameter_space['pool_size']),\n",
    "            'use_bottleneck': random.choice(hyperparameter_space['use_bottleneck']),\n",
    "            'bottleneck_size': random.choice(hyperparameter_space['bottleneck_size']),\n",
    "            'use_residual': random.choice(hyperparameter_space['use_residual']),\n",
    "            'use_attention': random.choice(hyperparameter_space['use_attention']),\n",
    "            'lstm_units': random.choice(hyperparameter_space['lstm_units']),\n",
    "            'dense_units': random.choice(hyperparameter_space['dense_units']),\n",
    "            'dropout_rate': random.choice(hyperparameter_space['dropout_rate']),\n",
    "            'learning_rate': random.choice(hyperparameter_space['learning_rate'])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(\"Parameters:\", params)\n",
    "        \n",
    "        # Build model with these hyperparameters\n",
    "        model = build_optimized_chord_model(\n",
    "            input_shape=dataset['num_features'],\n",
    "            num_classes=dataset['num_classes'],\n",
    "            window_size=dataset['window_size'],\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Simple callbacks for tuning (early stopping only)\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            dataset['X_train'], \n",
    "            dataset['y_train'],\n",
    "            epochs=epochs,\n",
    "            batch_size=256,\n",
    "            validation_data=(dataset['X_val'], dataset['y_val']),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        val_loss, val_acc = model.evaluate(dataset['X_val'], dataset['y_val'], verbose=0)\n",
    "        \n",
    "        # Record results\n",
    "        trial_result = {\n",
    "            'trial': trial + 1,\n",
    "            'params': params,\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history.history\n",
    "        }\n",
    "        trials_results.append(trial_result)\n",
    "        \n",
    "        print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = params\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\nHyperparameter Tuning Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Visualize trials results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort trials by accuracy\n",
    "    sorted_trials = sorted(trials_results, key=lambda x: x['val_acc'])\n",
    "    \n",
    "    trial_indices = [t['trial'] for t in sorted_trials]\n",
    "    accuracies = [t['val_acc'] for t in sorted_trials]\n",
    "    \n",
    "    plt.bar(range(len(trial_indices)), accuracies, color='skyblue')\n",
    "    plt.xlabel('Trial (sorted by accuracy)')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Hyperparameter Tuning Results')\n",
    "    plt.xticks(range(len(trial_indices)), trial_indices)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_params, trials_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning with a small number of trials for demonstration\n",
    "best_params, trials_results = hyperparameter_tuning(dataset, n_trials=HYPERPARAMETER_TRIALS, epochs=TRIAL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with best hyperparameters\n",
    "best_model = build_optimized_chord_model(\n",
    "    input_shape=dataset['num_features'],\n",
    "    num_classes=dataset['num_classes'],\n",
    "    window_size=dataset['window_size'],\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Create callbacks for the full training\n",
    "best_callbacks = create_model_callbacks(\n",
    "    checkpoint_path='models/best_chord_model.keras',\n",
    "    patience_early=15,\n",
    "    patience_lr=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(dataset, model, callbacks, epochs=EPOCHS, batch_size=256, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the final model on the full dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dictionary containing dataset splits and metadata\n",
    "        model: Model to train\n",
    "        callbacks: Callbacks for training\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Batch size for training\n",
    "        verbose: Verbosity level\n",
    "        \n",
    "    Returns:\n",
    "        History object\n",
    "    \"\"\"\n",
    "    print(\"Training final model...\")\n",
    "    \n",
    "    # Train model with best hyperparameters\n",
    "    history = model.fit(\n",
    "        dataset['X_train'],\n",
    "        dataset['y_train'],\n",
    "        validation_data=(dataset['X_val'], dataset['y_val']),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the final model\n",
    "history = train_final_model(dataset, best_model, best_callbacks, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history with detailed metrics.\n",
    "    \n",
    "    Args:\n",
    "        history: History object from model training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy distribution (last 10 epochs)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    last_epochs = min(10, len(history.history['accuracy']))\n",
    "    \n",
    "    train_acc = history.history['accuracy'][-last_epochs:]\n",
    "    val_acc = history.history['val_accuracy'][-last_epochs:]\n",
    "    \n",
    "    plt.boxplot([train_acc, val_acc], labels=['Train', 'Validation'])\n",
    "    plt.title(f'Accuracy Distribution (Last {last_epochs} Epochs)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate if available\n",
    "    if 'lr' in history.history:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.semilogy(history.history['lr'])\n",
    "        plt.title('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate (log scale)')\n",
    "        plt.grid(linestyle='--', alpha=0.3)\n",
    "    else:\n",
    "        # Plot epoch-wise improvement if learning rate not available\n",
    "        plt.subplot(2, 2, 4)\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        improvements = [val_acc[i] - val_acc[i-1] for i in range(1, len(val_acc))]\n",
    "        plt.bar(range(1, len(val_acc)), improvements, color='skyblue')\n",
    "        plt.axhline(y=0, color='red', linestyle='--')\n",
    "        plt.title('Epoch-wise Validation Accuracy Improvement')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Improvement')\n",
    "        plt.grid(linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "def save_training_artifacts(model, history, dataset, best_params, filepath_prefix='results/chord_recognition'):\n",
    "    \"\"\"\n",
    "    Save model, history, and metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        history: Training history\n",
    "        dataset: Dataset information\n",
    "        best_params: Best hyperparameters\n",
    "        filepath_prefix: Prefix for saved files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"{filepath_prefix}_model_{timestamp}.keras\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Save history\n",
    "    history_path = f\"{filepath_prefix}_history_{timestamp}.pkl\"\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"Training history saved to {history_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    encoder_path = f\"{filepath_prefix}_encoder_{timestamp}.pkl\"\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(dataset['label_encoder'], f)\n",
    "    print(f\"Label encoder saved to {encoder_path}\")\n",
    "    \n",
    "    # Save hyperparameters and metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'window_size': dataset['window_size'],\n",
    "        'hop_size': dataset['hop_size'],\n",
    "        'num_features': dataset['num_features'],\n",
    "        'num_classes': dataset['num_classes'],\n",
    "        'hyperparameters': best_params,\n",
    "        'final_val_accuracy': history.history['val_accuracy'][-1],\n",
    "        'final_val_loss': history.history['val_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    metadata_path = f\"{filepath_prefix}_metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': model_path,\n",
    "        'history_path': history_path,\n",
    "        'encoder_path': encoder_path,\n",
    "        'metadata_path': metadata_path\n",
    "    }\n",
    "\n",
    "# Save all training artifacts\n",
    "saved_paths = save_training_artifacts(best_model, history, dataset, best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
