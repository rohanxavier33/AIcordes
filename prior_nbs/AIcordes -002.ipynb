{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8a9b71",
   "metadata": {},
   "source": [
    "Deep Learning for Chord Recognition\n",
    "# McGill Billboard Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4cecf",
   "metadata": {},
   "source": [
    "This notebook implements a deep learning approach to chord recognition using the McGill Billboard dataset. The goal is to predict chord labels from audio chroma features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487aa419",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Introduction and Problem Description\n",
    "2. Data Exploration (EDA)\n",
    "3. Data Preparation\n",
    "4. Model Development\n",
    "5. Training and Evaluation\n",
    "6. Results and Analysis\n",
    "7. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3aba1",
   "metadata": {},
   "source": [
    "# 1. Introduction and Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351906e2",
   "metadata": {},
   "source": [
    "In this project, we implement a deep learning solution for automatic chord recognition from audio. Chord recognition is a fundamental task in music information retrieval (MIR) with applications in music education, transcription, recommendation systems, and musicological analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62016bce",
   "metadata": {},
   "source": [
    "We aim to develop and compare multiple neural network architectures for identifying chords from audio features, specifically using chroma representations derived from music audio files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba119d60",
   "metadata": {},
   "source": [
    "Our approach includes implementing memory-efficient data handling techniques to process a large dataset, and incorporating temporal context to improve chord recognition accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579d1ee",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5443862",
   "metadata": {},
   "source": [
    "We'll be using the McGill Billboard dataset, which contains:\n",
    "- Chroma features extracted from popular music recordings\n",
    "- Time-aligned chord labels\n",
    "- 890 songs spanning several decades of popular music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, Input, BatchNormalization, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.colors as mcolors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ca9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 15  # Number of frames to use for context window\n",
    "STEP_SIZE = 1  # Step size for creating sequences\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1194e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "data_dir = Path('data/McGill-Billboard')\n",
    "chordino_dir = data_dir / 'chordino'\n",
    "lab_dir = data_dir / 'lab'\n",
    "annotations_dir = data_dir / 'annotations'\n",
    "index_path = data_dir / 'index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfdbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "random.seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for models and results\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c3a68",
   "metadata": {},
   "source": [
    "# 2. Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bb5d0",
   "metadata": {},
   "source": [
    "Let's first explore the dataset to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset index\n",
    "index_df = pd.read_csv(index_path)\n",
    "print(f\"Total entries in index: {len(index_df)}\")\n",
    "print(f\"Entries with complete data: {index_df['title'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entries with missing/incomplete/unavailable data\n",
    "index_df = index_df.dropna(subset=['title'])\n",
    "print(f'Total Entries after cleaning: {len(index_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492679e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(f\"Number of unique songs: {index_df['title'].nunique()}\")\n",
    "print(f\"Number of unique artists: {index_df['artist'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chart_date to datetime\n",
    "index_df['chart_date'] = pd.to_datetime(index_df['chart_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19563398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and decade\n",
    "index_df['year'] = index_df['chart_date'].dt.year\n",
    "index_df['decade'] = (index_df['year'] // 10) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution by decade\n",
    "plt.figure(figsize=(10, 6))\n",
    "decade_counts = index_df['decade'].value_counts().sort_index()\n",
    "decade_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Songs by Decade')\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available directories\n",
    "song_ids = [d.name for d in chordino_dir.iterdir() if d.is_dir()]\n",
    "print(f\"Number of songs with chroma features: {len(song_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270faa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the same songs have lab files\n",
    "lab_ids = [d.name for d in lab_dir.iterdir() if d.is_dir()]\n",
    "print(f\"Number of songs with lab files: {len(lab_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b46a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find songs that have both chroma and labels\n",
    "common_ids = set(song_ids).intersection(set(lab_ids))\n",
    "print(f\"Number of songs with both features and labels: {len(common_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b42552",
   "metadata": {},
   "source": [
    "## Exploring a Random Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6f036",
   "metadata": {},
   "source": [
    "Let's examine a random song from the dataset to understand the structure of the chroma features and chord labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d133ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random example\n",
    "example_id = list(common_ids)[random.randint(0, len(common_ids) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d72d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chroma features for the example song\n",
    "chroma_path = chordino_dir / example_id / 'bothchroma.csv'\n",
    "tuning_path = chordino_dir / example_id / 'tuning.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chroma_path.exists() and tuning_path.exists():\n",
    "    chroma_data = pd.read_csv(chroma_path, header=None)\n",
    "    tuning_data = pd.read_csv(tuning_path, header=None)\n",
    "    \n",
    "    print(f\"\\nChroma shape for song {example_id}: {chroma_data.shape}\")\n",
    "    print(f\"Tuning shape for song {example_id}: {tuning_data.shape}\")\n",
    "    \n",
    "    # Display a sample of the chroma data\n",
    "    print(\"\\nSample of chroma data:\")\n",
    "    print(chroma_data.head())\n",
    "    \n",
    "    # Extract chroma values (assuming columns 2-13 contain the chroma values)\n",
    "    chroma_values = chroma_data.iloc[:, 2:14].values\n",
    "    if chroma_values.shape[1] != 12:\n",
    "        # If the format is different, try to find the 12 consecutive columns with numeric data\n",
    "        for i in range(len(chroma_data.columns) - 12 + 1):\n",
    "            test_values = chroma_data.iloc[:, i:i+12].values\n",
    "            if np.issubdtype(test_values.dtype, np.number):\n",
    "                chroma_values = test_values\n",
    "                print(f\"Found chroma values in columns {i} to {i+11}\")\n",
    "                break\n",
    "    \n",
    "    # Plot chroma data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(211)\n",
    "    sns.heatmap(chroma_values[:100, :].T, cmap='coolwarm', cbar=True)\n",
    "    plt.title(f'Chroma Feature Heatmap for First 100 Frames - Song {example_id}')\n",
    "    plt.xlabel('Time frames')\n",
    "    plt.ylabel('Pitch Class')\n",
    "    \n",
    "    # Add pitch class labels on the y-axis\n",
    "    pitch_classes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    plt.yticks(np.arange(12) + 0.5, pitch_classes)\n",
    "    \n",
    "    # Load chord labels for the example song\n",
    "    lab_path = lab_dir / example_id / \"full.lab\"\n",
    "    if lab_path.exists():\n",
    "        # Read the lab file (tab-separated with no header)\n",
    "        lab_data = pd.read_csv(lab_path, sep='\\t', header=None, names=['start_time', 'end_time', 'chord'])\n",
    "        \n",
    "        print(f\"\\nNumber of chord segments for song {example_id}: {len(lab_data)}\")\n",
    "        print(\"\\nSample of chord labels:\")\n",
    "        print(lab_data.head())\n",
    "        \n",
    "        # Count the unique chords in this song\n",
    "        print(f\"\\nNumber of unique chords in song {example_id}: {lab_data['chord'].nunique()}\")\n",
    "        print(\"\\nMost common chords:\")\n",
    "        print(lab_data['chord'].value_counts().head(10))\n",
    "        \n",
    "        # Plot chord durations\n",
    "        plt.subplot(212)\n",
    "        chord_durations = lab_data['end_time'] - lab_data['start_time']\n",
    "        plt.hist(chord_durations, bins=30, alpha=0.7, color='teal')\n",
    "        plt.title('Chord Duration Distribution')\n",
    "        plt.xlabel('Duration (seconds)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot most common chords\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_chords = lab_data['chord'].value_counts().head(15)\n",
    "        top_chords.plot(kind='bar', color='coral')\n",
    "        plt.title(f'Most Common Chords in Song {example_id}')\n",
    "        plt.xlabel('Chord')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Lab file not found for song {example_id}\")\n",
    "else:\n",
    "    print(f\"Chroma or tuning data not found for song {example_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56710649",
   "metadata": {},
   "source": [
    "## Analyzing Chord Distribution Across the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1346168",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_chord_distribution(song_ids, top_n=20):\n",
    "    \"\"\"Get the distribution of chords across all songs\"\"\"\n",
    "    all_chords = Counter()\n",
    "    processed_songs = 0\n",
    "    \n",
    "    for song_id in tqdm(song_ids, desc=\"Analyzing chord distribution\"):\n",
    "        lab_path = lab_dir / song_id / \"full.lab\"\n",
    "        \n",
    "        if lab_path.exists():\n",
    "            # Load and count chord labels\n",
    "            lab_data = pd.read_csv(lab_path, sep='\\t', header=None, names=['start_time', 'end_time', 'chord'])\n",
    "            chord_counts = lab_data['chord'].value_counts().to_dict()\n",
    "            \n",
    "            # Add to overall counter\n",
    "            all_chords.update(chord_counts)\n",
    "            processed_songs += 1\n",
    "    \n",
    "    print(f\"Processed {processed_songs} songs for chord distribution\")\n",
    "    return all_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c53a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chord distribution across a sample of songs (limit to 100 for speed)\n",
    "chord_distribution = get_chord_distribution(list(common_ids)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top chords\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_chords = pd.Series(dict(chord_distribution.most_common(30)))\n",
    "top_chords.plot(kind='bar', color='lightseagreen')\n",
    "plt.title('Most Common Chords Across the Dataset (Top 30)')\n",
    "plt.xlabel('Chord')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019d248",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "total_chords = sum(chord_distribution.values())\n",
    "unique_chords = len(chord_distribution)\n",
    "print(f\"Total chord occurrences: {total_chords}\")\n",
    "print(f\"Number of unique chords: {unique_chords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a830ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# How much of the dataset is covered by the top N chords?\n",
    "def coverage_analysis(chord_distribution, tops=[10, 20, 50, 100, 200]):\n",
    "    total = sum(chord_distribution.values())\n",
    "    for n in tops:\n",
    "        top_n = sum(dict(chord_distribution.most_common(n)).values())\n",
    "        print(f\"Top {n} chords cover {top_n/total*100:.2f}% of all chord occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_analysis(chord_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19d18f",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9730ca",
   "metadata": {},
   "source": [
    "Now let's implement memory-efficient data handling techniques to process the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662776d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def normalize_chroma_tuning(chroma_array, song_id):\n",
    "    \"\"\"Normalize chroma features based on tuning information\"\"\"\n",
    "    tuning_path = chordino_dir / song_id / 'tuning.csv'\n",
    "    \n",
    "    if not tuning_path.exists():\n",
    "        return chroma_array  # Can't normalize, return original\n",
    "    \n",
    "    try:\n",
    "        # Load tuning information\n",
    "        tuning_data = pd.read_csv(tuning_path, header=None)\n",
    "        tuning_frequency = float(tuning_data.iloc[0, 3])  # Assuming 4th column has the frequency\n",
    "        \n",
    "        # Calculate tuning deviation in semitones\n",
    "        # A4 = 440Hz in standard tuning\n",
    "        tuning_deviation = 12 * np.log2(tuning_frequency / 440.0)\n",
    "        \n",
    "        # No need to adjust if very close to standard tuning\n",
    "        if abs(tuning_deviation) < 0.01:\n",
    "            return chroma_array\n",
    "            \n",
    "        # Convert deviation to circular shift (positive means shift right)\n",
    "        # We negate because if tuning is higher, we need to shift chroma left\n",
    "        shift_amount = -tuning_deviation\n",
    "        \n",
    "        # Apply circular shift to each chroma vector\n",
    "        normalized_chroma = np.zeros_like(chroma_array)\n",
    "        for i in range(len(chroma_array)):\n",
    "            # Use interpolation for fractional shifts\n",
    "            for j in range(12):\n",
    "                # Calculate the fractional bin position after shifting\n",
    "                bin_pos = (j + shift_amount) % 12\n",
    "                bin_idx = int(bin_pos)\n",
    "                bin_frac = bin_pos - bin_idx\n",
    "                \n",
    "                # Distribute energy between adjacent bins (linear interpolation)\n",
    "                normalized_chroma[i, bin_idx % 12] += chroma_array[i, j] * (1 - bin_frac)\n",
    "                normalized_chroma[i, (bin_idx + 1) % 12] += chroma_array[i, j] * bin_frac\n",
    "        \n",
    "        return normalized_chroma\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing chroma for song {song_id}: {e}\")\n",
    "        return chroma_array  # Return original on error\n",
    "\n",
    "def load_chroma(song_id):\n",
    "    \"\"\"Load chroma features for a given song ID with robust error handling\"\"\"\n",
    "    chroma_path = chordino_dir / song_id / 'bothchroma.csv'\n",
    "    \n",
    "    if not chroma_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read the file\n",
    "        data = pd.read_csv(chroma_path, header=None)\n",
    "        \n",
    "        # Figure out which columns contain the chroma data (should be 12 consecutive numeric columns)\n",
    "        # Start by converting all to numeric, with errors='coerce' to handle non-numeric values\n",
    "        numeric_data = data.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Find the column indices where most values are not NaN (these should be our chroma columns)\n",
    "        valid_columns = numeric_data.count() > 0.8 * len(numeric_data)\n",
    "        valid_column_indices = valid_columns[valid_columns].index.tolist()\n",
    "        \n",
    "        # Find a consecutive sequence of 12 columns (for 12 pitch classes)\n",
    "        chroma_start = None\n",
    "        for i in range(len(valid_column_indices) - 11):\n",
    "            if valid_column_indices[i:i+12] == list(range(valid_column_indices[i], valid_column_indices[i] + 12)):\n",
    "                chroma_start = valid_column_indices[i]\n",
    "                break\n",
    "        \n",
    "        if chroma_start is None:\n",
    "            # If no perfect consecutive sequence, try to find the best 12 consecutive columns\n",
    "            for i in range(len(data.columns) - 11):\n",
    "                test_cols = data.iloc[:, i:i+12]\n",
    "                if test_cols.apply(pd.to_numeric, errors='coerce').count().min() > 0.8 * len(data):\n",
    "                    chroma_start = i\n",
    "                    break\n",
    "        \n",
    "        if chroma_start is None:\n",
    "            # If still no luck, just take columns 2-13 (common format)\n",
    "            chroma_start = 2\n",
    "        \n",
    "        # Extract and convert the chroma values\n",
    "        chroma_values = data.iloc[:, chroma_start:chroma_start+12].apply(pd.to_numeric, errors='coerce').values\n",
    "        \n",
    "        # Replace any remaining NaNs with zeros\n",
    "        chroma_values = np.nan_to_num(chroma_values)\n",
    "        \n",
    "        # Apply tuning normalization\n",
    "        chroma_values = normalize_chroma_tuning(chroma_values, song_id)\n",
    "        \n",
    "        return chroma_values.astype(np.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chroma for song {song_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_chord_labels(song_id):\n",
    "    \"\"\"Load chord labels for a given song ID\"\"\"\n",
    "    lab_path = lab_dir / song_id / \"full.lab\"\n",
    "    \n",
    "    if not lab_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the chord labels\n",
    "        lab_data = pd.read_csv(lab_path, sep='\\t', header=None, names=['start_time', 'end_time', 'chord'])\n",
    "        return lab_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chord labels for song {song_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def align_chroma_with_chords(chroma_array, chord_data, chroma_hop_size=0.01):\n",
    "    \"\"\"Align chroma features with chord labels\"\"\"\n",
    "    # Generate timestamps for each chroma frame\n",
    "    chroma_times = np.arange(len(chroma_array)) * chroma_hop_size\n",
    "    \n",
    "    aligned_chords = []\n",
    "    \n",
    "    for i, time in enumerate(chroma_times):\n",
    "        # Find the chord label for this time point\n",
    "        matching_chords = chord_data[(chord_data['start_time'] <= time) & (chord_data['end_time'] > time)]\n",
    "        \n",
    "        if len(matching_chords) > 0:\n",
    "            chord = matching_chords.iloc[0]['chord']\n",
    "        else:\n",
    "            chord = \"N\"  # No chord (silence or undefined)\n",
    "        \n",
    "        aligned_chords.append(chord)\n",
    "    \n",
    "    return aligned_chords\n",
    "\n",
    "def create_sequences(features, labels, seq_length=15, step=1):\n",
    "    \"\"\"Create sequences for temporal context\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(features) - seq_length + 1, step):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(labels[i+seq_length-1])  # Target is the label of the last frame in the sequence\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ea0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tuning distribution across the dataset\n",
    "def analyze_tuning_distribution():\n",
    "    \"\"\"Analyze the distribution of tuning frequencies across the dataset\"\"\"\n",
    "    tuning_frequencies = []\n",
    "    song_ids = []\n",
    "    \n",
    "    for song_id in tqdm(common_ids, desc=\"Analyzing tuning\"):\n",
    "        tuning_path = chordino_dir / song_id / 'tuning.csv'\n",
    "        if tuning_path.exists():\n",
    "            try:\n",
    "                tuning_data = pd.read_csv(tuning_path, header=None)\n",
    "                tuning_freq = float(tuning_data.iloc[0, 3])\n",
    "                tuning_frequencies.append(tuning_freq)\n",
    "                song_ids.append(song_id)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Calculate deviation in cents (100 cents = 1 semitone)\n",
    "    tuning_deviations = [1200 * np.log2(freq / 440.0) for freq in tuning_frequencies]\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(tuning_deviations, bins=50, color='teal', alpha=0.7)\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='A = 440 Hz')\n",
    "    plt.xlabel('Tuning Deviation (cents)')\n",
    "    plt.ylabel('Number of Songs')\n",
    "    plt.title('Distribution of Tuning Deviations in the Dataset')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot histogram of absolute deviations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist([abs(dev) for dev in tuning_deviations], bins=30, color='coral', alpha=0.7)\n",
    "    plt.xlabel('Absolute Tuning Deviation (cents)')\n",
    "    plt.ylabel('Number of Songs')\n",
    "    plt.title('Distribution of Absolute Tuning Deviations')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Average tuning frequency: {np.mean(tuning_frequencies):.2f} Hz\")\n",
    "    print(f\"Median tuning frequency: {np.median(tuning_frequencies):.2f} Hz\")\n",
    "    print(f\"Minimum tuning frequency: {np.min(tuning_frequencies):.2f} Hz\")\n",
    "    print(f\"Maximum tuning frequency: {np.max(tuning_frequencies):.2f} Hz\")\n",
    "    print(f\"Standard deviation: {np.std(tuning_frequencies):.2f} Hz\")\n",
    "    \n",
    "    # Calculate percentage of songs with significant tuning deviations\n",
    "    significant_deviation_count = sum(1 for dev in tuning_deviations if abs(dev) > 30)\n",
    "    print(f\"Songs with significant tuning deviation (>30 cents): {significant_deviation_count} ({significant_deviation_count/len(tuning_frequencies)*100:.1f}%)\")\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = [(song_id, freq, dev) for song_id, freq, dev in \n",
    "                zip(song_ids, tuning_frequencies, tuning_deviations) \n",
    "                if abs(dev) > 50]  # More than 50 cents (half semitone) deviation\n",
    "    \n",
    "    if outliers:\n",
    "        print(\"\\nSignificant tuning deviations (>50 cents):\")\n",
    "        for song_id, freq, dev in sorted(outliers, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "            print(f\"Song {song_id}: {freq:.2f} Hz ({dev:.1f} cents)\")\n",
    "    \n",
    "    return tuning_frequencies, tuning_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of tuning normalization on a specific example\n",
    "def visualize_tuning_normalization(song_id=None):\n",
    "    \"\"\"Visualize the effect of tuning normalization on chroma features\"\"\"\n",
    "    if song_id is None:\n",
    "        # Find a song with significant tuning deviation\n",
    "        for s_id in common_ids:\n",
    "            tuning_path = chordino_dir / s_id / 'tuning.csv'\n",
    "            if tuning_path.exists():\n",
    "                try:\n",
    "                    tuning_data = pd.read_csv(tuning_path, header=None)\n",
    "                    tuning_freq = float(tuning_data.iloc[0, 3])\n",
    "                    deviation = 1200 * np.log2(tuning_freq / 440.0)\n",
    "                    if abs(deviation) > 30:  # More than 30 cents deviation\n",
    "                        song_id = s_id\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    if song_id is None:\n",
    "        print(\"Could not find a song with significant tuning deviation for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Load the chroma features without normalization\n",
    "    chroma_path = chordino_dir / song_id / 'bothchroma.csv'\n",
    "    if not chroma_path.exists():\n",
    "        print(f\"Chroma file not found for song {song_id}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Read the file\n",
    "        data = pd.read_csv(chroma_path, header=None)\n",
    "        \n",
    "        # Figure out which columns contain the chroma data\n",
    "        numeric_data = data.apply(pd.to_numeric, errors='coerce')\n",
    "        valid_columns = numeric_data.count() > 0.8 * len(numeric_data)\n",
    "        valid_column_indices = valid_columns[valid_columns].index.tolist()\n",
    "        \n",
    "        # Find chroma columns\n",
    "        chroma_start = None\n",
    "        for i in range(len(valid_column_indices) - 11):\n",
    "            if valid_column_indices[i:i+12] == list(range(valid_column_indices[i], valid_column_indices[i] + 12)):\n",
    "                chroma_start = valid_column_indices[i]\n",
    "                break\n",
    "        \n",
    "        if chroma_start is None:\n",
    "            for i in range(len(data.columns) - 11):\n",
    "                test_cols = data.iloc[:, i:i+12]\n",
    "                if test_cols.apply(pd.to_numeric, errors='coerce').count().min() > 0.8 * len(data):\n",
    "                    chroma_start = i\n",
    "                    break\n",
    "        \n",
    "        if chroma_start is None:\n",
    "            chroma_start = 2\n",
    "        \n",
    "        # Extract raw chroma values\n",
    "        raw_chroma = data.iloc[:, chroma_start:chroma_start+12].apply(pd.to_numeric, errors='coerce').values\n",
    "        raw_chroma = np.nan_to_num(raw_chroma)\n",
    "        \n",
    "        # Get the normalized chroma\n",
    "        normalized_chroma = normalize_chroma_tuning(raw_chroma, song_id)\n",
    "        \n",
    "        # Get tuning information\n",
    "        tuning_path = chordino_dir / song_id / 'tuning.csv'\n",
    "        tuning_data = pd.read_csv(tuning_path, header=None)\n",
    "        tuning_freq = float(tuning_data.iloc[0, 3])\n",
    "        deviation = 12 * np.log2(tuning_freq / 440.0)\n",
    "        \n",
    "        # Visualize both raw and normalized chroma\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot raw chroma\n",
    "        plt.subplot(211)\n",
    "        frame_range = min(100, len(raw_chroma))  # Show first 100 frames or less\n",
    "        sns.heatmap(raw_chroma[:frame_range].T, cmap='coolwarm')\n",
    "        plt.title(f'Raw Chroma Features - Song {song_id} (Tuning: {tuning_freq:.1f} Hz, {deviation*100:.1f} cents off standard)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Pitch Class')\n",
    "        plt.yticks(np.arange(12) + 0.5, ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'])\n",
    "        \n",
    "        # Plot normalized chroma\n",
    "        plt.subplot(212)\n",
    "        sns.heatmap(normalized_chroma[:frame_range].T, cmap='coolwarm')\n",
    "        plt.title('Normalized Chroma Features (Tuning Correction Applied)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Pitch Class')\n",
    "        plt.yticks(np.arange(12) + 0.5, ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return raw_chroma, normalized_chroma\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing tuning normalization for song {song_id}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953adf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tuning analysis\n",
    "print(\"Analyzing tuning variations across the dataset...\")\n",
    "tuning_frequencies, tuning_deviations = analyze_tuning_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf844ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of tuning normalization on an example song\n",
    "print(\"\\nVisualizing the effect of tuning normalization on an example song...\")\n",
    "raw_chroma, normalized_chroma = visualize_tuning_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671672c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ChordDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom data generator for memory-efficient loading\"\"\"\n",
    "    def __init__(self, song_ids, batch_size=32, seq_length=15, step=1, shuffle=True,\n",
    "                 label_encoder=None, is_training=True, num_classes=None):\n",
    "        self.song_ids = song_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.step = step\n",
    "        self.shuffle = shuffle\n",
    "        self.is_training = is_training\n",
    "        self.label_encoder = label_encoder\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Pre-load song lengths to calculate total size\n",
    "        self.song_data = []\n",
    "        total_frames = 0\n",
    "        \n",
    "        for song_id in tqdm(song_ids, desc=\"Analyzing songs for generator\"):\n",
    "            chroma = load_chroma(song_id)\n",
    "            chords = load_chord_labels(song_id)\n",
    "            \n",
    "            if chroma is not None and chords is not None:\n",
    "                # Calculate number of sequences in this song\n",
    "                num_seqs = max(0, (len(chroma) - seq_length + 1 + step - 1) // step)\n",
    "                if num_seqs > 0:\n",
    "                    total_frames += num_seqs\n",
    "                    self.song_data.append({\n",
    "                        'song_id': song_id,\n",
    "                        'num_sequences': num_seqs\n",
    "                    })\n",
    "        \n",
    "        self.total_sequences = total_frames\n",
    "        print(f\"Data generator initialized with {len(self.song_data)} songs and {self.total_sequences} total sequences\")\n",
    "        \n",
    "        # Create an array of sequence indices for shuffling\n",
    "        self.indices = np.arange(self.total_sequences)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches\"\"\"\n",
    "        return (self.total_sequences + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices at the end of each epoch if shuffle is True\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Calculate start and end indices for this batch\n",
    "        batch_start = batch_idx * self.batch_size\n",
    "        batch_end = min((batch_idx + 1) * self.batch_size, self.total_sequences)\n",
    "        \n",
    "        # Get the sequence indices for this batch\n",
    "        batch_indices = self.indices[batch_start:batch_end]\n",
    "        \n",
    "        # Initialize batch arrays\n",
    "        batch_size = batch_end - batch_start\n",
    "        X_batch = np.zeros((batch_size, self.seq_length, 12), dtype=np.float32)\n",
    "        y_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        \n",
    "        # Load the data for this batch\n",
    "        for i, seq_idx in enumerate(batch_indices):\n",
    "            # Find which song this sequence belongs to\n",
    "            song_offset = 0\n",
    "            for song_data in self.song_data:\n",
    "                if seq_idx < song_offset + song_data['num_sequences']:\n",
    "                    song_id = song_data['song_id']\n",
    "                    sequence_in_song = seq_idx - song_offset\n",
    "                    \n",
    "                    # Calculate the starting frame in the song\n",
    "                    frame_start = sequence_in_song * self.step\n",
    "                    \n",
    "                    # Load the song data if not already cached\n",
    "                    chroma = load_chroma(song_id)\n",
    "                    chords_data = load_chord_labels(song_id)\n",
    "                    \n",
    "                    if chroma is not None and chords_data is not None:\n",
    "                        # Align chords with chroma\n",
    "                        aligned_chords = align_chroma_with_chords(chroma, chords_data)\n",
    "                        \n",
    "                        # Extract the sequence\n",
    "                        seq_features = chroma[frame_start:frame_start+self.seq_length]\n",
    "                        seq_label = aligned_chords[frame_start+self.seq_length-1]\n",
    "                        \n",
    "                        # Ensure we have a full sequence (handle edge cases)\n",
    "                        if len(seq_features) == self.seq_length:\n",
    "                            X_batch[i] = seq_features\n",
    "                            \n",
    "                            # Encode the label\n",
    "                            if self.label_encoder is not None and seq_label in self.label_encoder.classes_:\n",
    "                                y_batch[i] = self.label_encoder.transform([seq_label])[0]\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                song_offset += song_data['num_sequences']\n",
    "        \n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751ce7c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(song_ids, test_size=0.1, val_size=0.1, seed=42):\n",
    "    \"\"\"Prepare the dataset for training, validation, and testing\"\"\"\n",
    "    # Shuffle and split the song IDs\n",
    "    random.seed(seed)\n",
    "    song_ids = list(song_ids)\n",
    "    random.shuffle(song_ids)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    test_idx = int(len(song_ids) * test_size)\n",
    "    val_idx = int(len(song_ids) * (test_size + val_size))\n",
    "    \n",
    "    # Split the song IDs\n",
    "    test_song_ids = song_ids[:test_idx]\n",
    "    val_song_ids = song_ids[test_idx:val_idx]\n",
    "    train_song_ids = song_ids[val_idx:]\n",
    "    \n",
    "    print(f\"Split {len(song_ids)} songs into {len(train_song_ids)} training, {len(val_song_ids)} validation, and {len(test_song_ids)} test songs\")\n",
    "    \n",
    "    return train_song_ids, val_song_ids, test_song_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f158a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_label_encoder(song_ids, max_songs=None):\n",
    "    \"\"\"Create a label encoder for the chord labels\"\"\"\n",
    "    all_chords = set()\n",
    "    processed = 0\n",
    "    \n",
    "    for song_id in tqdm(song_ids, desc=\"Creating label encoder\"):\n",
    "        if max_songs is not None and processed >= max_songs:\n",
    "            break\n",
    "            \n",
    "        chord_data = load_chord_labels(song_id)\n",
    "        if chord_data is not None:\n",
    "            all_chords.update(chord_data['chord'].unique())\n",
    "            processed += 1\n",
    "    \n",
    "    print(f\"Found {len(all_chords)} unique chords across {processed} songs\")\n",
    "    \n",
    "    # Create and fit the label encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(all_chords))\n",
    "    \n",
    "    return encoder, list(all_chords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eeb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_songs, val_songs, test_songs = prepare_data(common_ids, test_size=TEST_SPLIT, val_size=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e28223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder\n",
    "label_encoder, unique_chords = create_label_encoder(train_songs)\n",
    "num_classes = len(unique_chords)\n",
    "print(f\"Number of chord classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the label encoder for later use\n",
    "with open('models/chord_label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f671867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = ChordDataGenerator(\n",
    "    train_songs, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    seq_length=SEQUENCE_LENGTH, \n",
    "    step=STEP_SIZE,\n",
    "    label_encoder=label_encoder,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e16fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = ChordDataGenerator(\n",
    "    val_songs, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    seq_length=SEQUENCE_LENGTH, \n",
    "    step=STEP_SIZE,\n",
    "    shuffle=False,\n",
    "    label_encoder=label_encoder,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174da7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ChordDataGenerator(\n",
    "    test_songs, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    seq_length=SEQUENCE_LENGTH, \n",
    "    step=STEP_SIZE,\n",
    "    shuffle=False,\n",
    "    label_encoder=label_encoder,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e63fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the most common chord classes\n",
    "top_chord_counts = Counter()\n",
    "for song_id in tqdm(train_songs[:50], desc=\"Analyzing common chords\"):\n",
    "    chord_data = load_chord_labels(song_id)\n",
    "    if chord_data is not None:\n",
    "        chord_counts = chord_data['chord'].value_counts().to_dict()\n",
    "        top_chord_counts.update(chord_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 20 chord classes in the dataset:\")\n",
    "for chord, count in top_chord_counts.most_common(20):\n",
    "    chord_idx = label_encoder.transform([chord])[0]\n",
    "    print(f\"{chord} (class {chord_idx}): {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e6c5c",
   "metadata": {},
   "source": [
    "# 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df42c0",
   "metadata": {},
   "source": [
    "Let's develop several neural network architectures for chord recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1149fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Create a CNN model for chord recognition\"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # First conv block\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second conv block\n",
    "        Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93401224",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Create an LSTM model for chord recognition\"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # LSTM layers\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.25),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089a55e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_hybrid_model(input_shape, num_classes):\n",
    "    \"\"\"Create a hybrid CNN-LSTM model for chord recognition\"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # LSTM for temporal dependencies\n",
    "        Bidirectional(LSTM(128, return_sequences=False)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4218129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape based on sequence length and features\n",
    "input_shape = (SEQUENCE_LENGTH, 12)  # 12 chroma features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "lstm_model = create_lstm_model(input_shape, num_classes)\n",
    "hybrid_model = create_hybrid_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summaries\n",
    "print(\"CNN Model Summary:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLSTM Model Summary:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHybrid CNN-LSTM Model Summary:\")\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165f229",
   "metadata": {},
   "source": [
    "# 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354c183",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "def get_callbacks(model_name):\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'models/{model_name}_best.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model\n",
    "print(\"Training CNN Model...\")\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=get_callbacks('cnn'),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09758f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=get_callbacks('lstm'),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9187b9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train the hybrid model\n",
    "print(\"Training Hybrid CNN-LSTM Model...\")\n",
    "hybrid_history = hybrid_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=get_callbacks('hybrid'),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81f74a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "def plot_history(histories, names):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for history, name in zip(histories, names):\n",
    "        plt.plot(history.history['accuracy'], label=f'{name} Train')\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{name} Val')\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for history, name in zip(histories, names):\n",
    "        plt.plot(history.history['loss'], label=f'{name} Train')\n",
    "        plt.plot(history.history['val_loss'], label=f'{name} Val')\n",
    "    \n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training histories\n",
    "plot_history(\n",
    "    [cnn_history, lstm_history, hybrid_history],\n",
    "    ['CNN', 'LSTM', 'Hybrid']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the importance of tuning normalization \n",
    "# by comparing performance with and without it\n",
    "def compare_tuning_normalization_impact():\n",
    "    \"\"\"Compare model performance with and without tuning normalization\"\"\"\n",
    "    # We'll use a smaller subset of data for this experiment\n",
    "    print(\"Comparing the impact of tuning normalization...\")\n",
    "    \n",
    "    # First, let's modify the data generator to allow disabling tuning normalization\n",
    "    class TuningExperimentGenerator(tf.keras.utils.Sequence):\n",
    "        \"\"\"Custom generator that can toggle tuning normalization\"\"\"\n",
    "        def __init__(self, song_ids, batch_size=32, seq_length=15, apply_normalization=True,\n",
    "                     label_encoder=None, shuffle=True, max_songs=50):\n",
    "            self.song_ids = song_ids[:max_songs]  # Limit number of songs for quick experiment\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_length = seq_length\n",
    "            self.apply_normalization = apply_normalization\n",
    "            self.label_encoder = label_encoder\n",
    "            self.shuffle = shuffle\n",
    "            \n",
    "            # Pre-load data for this experiment\n",
    "            self.all_sequences = []\n",
    "            self.all_labels = []\n",
    "            \n",
    "            for song_id in tqdm(self.song_ids, desc=f\"Loading data (norm={apply_normalization})\"):\n",
    "                # Load chroma without normalization first\n",
    "                chroma_path = chordino_dir / song_id / 'bothchroma.csv'\n",
    "                if not chroma_path.exists():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Read the file\n",
    "                    data = pd.read_csv(chroma_path, header=None)\n",
    "                    \n",
    "                    # Extract chroma values (simplified version)\n",
    "                    numeric_data = data.apply(pd.to_numeric, errors='coerce')\n",
    "                    chroma_values = None\n",
    "                    \n",
    "                    # Try to find chroma columns\n",
    "                    for i in range(len(data.columns) - 11):\n",
    "                        test_cols = data.iloc[:, i:i+12]\n",
    "                        if test_cols.apply(pd.to_numeric, errors='coerce').count().min() > 0.8 * len(data):\n",
    "                            chroma_values = test_cols.apply(pd.to_numeric, errors='coerce').values\n",
    "                            break\n",
    "                    \n",
    "                    if chroma_values is None:\n",
    "                        chroma_values = data.iloc[:, 2:14].apply(pd.to_numeric, errors='coerce').values\n",
    "                    \n",
    "                    # Replace NaNs with zeros\n",
    "                    chroma_values = np.nan_to_num(chroma_values)\n",
    "                    \n",
    "                    # Apply normalization if specified\n",
    "                    if self.apply_normalization:\n",
    "                        chroma_values = normalize_chroma_tuning(chroma_values, song_id)\n",
    "                    \n",
    "                    # Load chord labels\n",
    "                    lab_path = lab_dir / song_id / \"full.lab\"\n",
    "                    if not lab_path.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    chord_data = pd.read_csv(lab_path, sep='\\t', header=None, names=['start_time', 'end_time', 'chord'])\n",
    "                    \n",
    "                    # Align chords with chroma\n",
    "                    aligned_chords = align_chroma_with_chords(chroma_values, chord_data)\n",
    "                    \n",
    "                    # Create sequences\n",
    "                    for i in range(0, len(chroma_values) - self.seq_length + 1, 10):  # Use a larger step size for efficiency\n",
    "                        self.all_sequences.append(chroma_values[i:i+self.seq_length])\n",
    "                        chord = aligned_chords[i+self.seq_length-1]\n",
    "                        \n",
    "                        # Encode the chord label\n",
    "                        if self.label_encoder is not None and chord in self.label_encoder.classes_:\n",
    "                            self.all_labels.append(self.label_encoder.transform([chord])[0])\n",
    "                        else:\n",
    "                            # Skip this sequence if label can't be encoded\n",
    "                            self.all_sequences.pop()\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing song {song_id}: {e}\")\n",
    "            \n",
    "            self.all_sequences = np.array(self.all_sequences)\n",
    "            self.all_labels = np.array(self.all_labels)\n",
    "            \n",
    "            # Create indices for shuffling\n",
    "            self.indices = np.arange(len(self.all_sequences))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "            \n",
    "            print(f\"Loaded {len(self.all_sequences)} sequences with normalization={self.apply_normalization}\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return (len(self.all_sequences) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Get indices for this batch\n",
    "            batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "            return self.all_sequences[batch_indices], self.all_labels[batch_indices]\n",
    "        \n",
    "        def on_epoch_end(self):\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "    \n",
    "    # Create generators with and without normalization\n",
    "    with_norm_gen = TuningExperimentGenerator(\n",
    "        song_ids=common_ids,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seq_length=SEQUENCE_LENGTH,\n",
    "        apply_normalization=True,\n",
    "        label_encoder=label_encoder,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    without_norm_gen = TuningExperimentGenerator(\n",
    "        song_ids=common_ids,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seq_length=SEQUENCE_LENGTH,\n",
    "        apply_normalization=False,\n",
    "        label_encoder=label_encoder,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create a simple model for quick evaluation\n",
    "    simple_model = Sequential([\n",
    "        Input(shape=(SEQUENCE_LENGTH, 12)),\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    simple_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate with normalization\n",
    "    print(\"\\nTraining with tuning normalization:\")\n",
    "    with_norm_history = simple_model.fit(\n",
    "        with_norm_gen,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reset model for fair comparison\n",
    "    simple_model.set_weights([np.random.normal(0, 0.01, w.shape) for w in simple_model.get_weights()])\n",
    "    \n",
    "    # Train and evaluate without normalization\n",
    "    print(\"\\nTraining without tuning normalization:\")\n",
    "    without_norm_history = simple_model.fit(\n",
    "        without_norm_gen,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(with_norm_history.history['accuracy'], label='With Tuning Normalization')\n",
    "    plt.plot(without_norm_history.history['accuracy'], label='Without Tuning Normalization')\n",
    "    plt.title('Effect of Tuning Normalization on Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(with_norm_history.history['loss'], label='With Tuning Normalization')\n",
    "    plt.plot(without_norm_history.history['loss'], label='Without Tuning Normalization')\n",
    "    plt.title('Effect of Tuning Normalization on Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"\\nFinal accuracy with tuning normalization: {with_norm_history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final accuracy without tuning normalization: {without_norm_history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Improvement from tuning normalization: {(with_norm_history.history['accuracy'][-1] - without_norm_history.history['accuracy'][-1])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison experiment\n",
    "compare_tuning_normalization_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test set\n",
    "print(\"Evaluating Models on Test Set...\")\n",
    "cnn_test_results = cnn_model.evaluate(test_generator, verbose=1)\n",
    "lstm_test_results = lstm_model.evaluate(test_generator, verbose=1)\n",
    "hybrid_test_results = hybrid_model.evaluate(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75210328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CNN Test Loss: {cnn_test_results[0]:.4f}, Test Accuracy: {cnn_test_results[1]:.4f}\")\n",
    "print(f\"LSTM Test Loss: {lstm_test_results[0]:.4f}, Test Accuracy: {lstm_test_results[1]:.4f}\")\n",
    "print(f\"Hybrid Test Loss: {hybrid_test_results[0]:.4f}, Test Accuracy: {hybrid_test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "cnn_model.save('models/cnn_model.h5')\n",
    "lstm_model.save('models/lstm_model.h5')\n",
    "hybrid_model.save('models/hybrid_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4680c8",
   "metadata": {},
   "source": [
    "# 6. Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e0a0d",
   "metadata": {},
   "source": [
    "Let's evaluate the models in more depth and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f80b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_model_predictions(model, generator, label_encoder, num_batches=10, class_threshold=20):\n",
    "    \"\"\"Analyze model predictions on a sample of data\"\"\"\n",
    "    # Generate predictions for a subset of the data\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(min(num_batches, len(generator))):\n",
    "        X_batch, y_batch = generator[i]\n",
    "        batch_preds = model.predict(X_batch, verbose=0)\n",
    "        batch_pred_classes = np.argmax(batch_preds, axis=1)\n",
    "        \n",
    "        y_true.extend(y_batch)\n",
    "        y_pred.extend(batch_pred_classes)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Get the class names\n",
    "    class_names = label_encoder.classes_\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Overall accuracy on {len(y_true)} samples: {accuracy:.4f}\")\n",
    "    \n",
    "    # Identify the most common chords in the test set\n",
    "    chord_counts = Counter(y_true)\n",
    "    most_common_chords = chord_counts.most_common()\n",
    "    \n",
    "    # Filter to chords with at least class_threshold occurrences\n",
    "    common_chords = [(chord, count) for chord, count in most_common_chords if count >= class_threshold]\n",
    "    print(f\"Found {len(common_chords)} chord classes with at least {class_threshold} occurrences\")\n",
    "    \n",
    "    # Calculate per-chord accuracy for common chords\n",
    "    chord_accuracies = {}\n",
    "    for chord_idx, count in common_chords:\n",
    "        mask = (y_true == chord_idx)\n",
    "        if np.sum(mask) > 0:\n",
    "            chord_acc = np.mean(y_pred[mask] == chord_idx)\n",
    "            chord_name = class_names[chord_idx]\n",
    "            chord_accuracies[chord_name] = (chord_acc, count)\n",
    "    \n",
    "    # Plot chord-specific accuracies\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    chords = []\n",
    "    accs = []\n",
    "    counts = []\n",
    "    \n",
    "    for chord, (acc, count) in sorted(chord_accuracies.items(), key=lambda x: x[1][0], reverse=True):\n",
    "        chords.append(chord)\n",
    "        accs.append(acc)\n",
    "        counts.append(count)\n",
    "    \n",
    "    # Create a colormap based on accuracies\n",
    "    colors = plt.cm.viridis(np.array(accs))\n",
    "    \n",
    "    ax = plt.bar(range(len(chords)), accs, color=colors)\n",
    "    plt.xticks(range(len(chords)), chords, rotation=90)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel('Chord')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Per-Chord Accuracy (for chords with at least {class_threshold} occurrences)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(0, 1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm)\n",
    "    cbar.set_label('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create confusion matrix for top N most common chords\n",
    "    N = min(15, len(common_chords))\n",
    "    top_n_chords = [chord for chord, _ in common_chords[:N]]\n",
    "    \n",
    "    # Filter to samples with these chords\n",
    "    mask = np.isin(y_true, top_n_chords)\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_n_chords)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=[class_names[idx] for idx in top_n_chords],\n",
    "                yticklabels=[class_names[idx] for idx in top_n_chords])\n",
    "    plt.xlabel('Predicted Chord')\n",
    "    plt.ylabel('True Chord')\n",
    "    plt.title(f'Confusion Matrix (Top {N} Most Common Chords)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, chord_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model based on validation accuracy\n",
    "# For this example, let's analyze the hybrid model\n",
    "print(\"Analyzing Hybrid Model Predictions...\")\n",
    "hybrid_accuracy, hybrid_chord_accuracies = analyze_model_predictions(\n",
    "    hybrid_model, test_generator, label_encoder, num_batches=20, class_threshold=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tuning_impact_by_genre():\n",
    "    \"\"\"Analyze if tuning variations correlate with certain genres or eras\"\"\"\n",
    "    # Load the tuning data with song IDs\n",
    "    tuning_data = []\n",
    "    for song_id in tqdm(common_ids, desc=\"Loading tuning data\"):\n",
    "        tuning_path = chordino_dir / song_id / 'tuning.csv'\n",
    "        if tuning_path.exists():\n",
    "            try:\n",
    "                tuning_df = pd.read_csv(tuning_path, header=None)\n",
    "                tuning_freq = float(tuning_df.iloc[0, 3])\n",
    "                deviation = 1200 * np.log2(tuning_freq / 440.0)\n",
    "                \n",
    "                # Get metadata for this song\n",
    "                song_meta = index_df[index_df['id'] == int(song_id)].iloc[0]\n",
    "                \n",
    "                tuning_data.append({\n",
    "                    'song_id': song_id,\n",
    "                    'title': song_meta['title'],\n",
    "                    'artist': song_meta['artist'],\n",
    "                    'year': song_meta['year'],\n",
    "                    'decade': song_meta['decade'],\n",
    "                    'tuning_freq': tuning_freq,\n",
    "                    'deviation_cents': deviation\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tuning for song {song_id}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    tuning_df = pd.DataFrame(tuning_data)\n",
    "    \n",
    "    # Analyze tuning by decade\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create a box plot of tuning deviations by decade\n",
    "    plt.subplot(211)\n",
    "    sns.boxplot(x='decade', y='deviation_cents', data=tuning_df)\n",
    "    plt.title('Tuning Deviations by Decade')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Deviation (cents)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Create a scatter plot of tuning deviations over time\n",
    "    plt.subplot(212)\n",
    "    plt.scatter(tuning_df['year'], tuning_df['deviation_cents'], \n",
    "                alpha=0.6, c=np.abs(tuning_df['deviation_cents']), cmap='viridis')\n",
    "    plt.colorbar(label='Absolute Deviation (cents)')\n",
    "    plt.title('Tuning Deviations Over Time')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Deviation (cents)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average absolute deviation by decade\n",
    "    decade_stats = tuning_df.groupby('decade')['deviation_cents'].agg(['mean', 'median', 'std', lambda x: np.mean(np.abs(x))])\n",
    "    decade_stats = decade_stats.rename(columns={'<lambda_0>': 'mean_abs'})\n",
    "    print(\"Tuning deviation statistics by decade:\")\n",
    "    print(decade_stats)\n",
    "    \n",
    "    # Check if any decades have significantly more tuning variation\n",
    "    \n",
    "    decade_groups = [tuning_df[tuning_df['decade'] == decade]['deviation_cents'].values \n",
    "                    for decade in tuning_df['decade'].unique()]\n",
    "    f_stat, p_value = f_oneway(*decade_groups)\n",
    "    \n",
    "    print(f\"\\nANOVA test for differences between decades:\")\n",
    "    print(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"There are statistically significant differences in tuning between decades.\")\n",
    "    else:\n",
    "        print(\"No statistically significant differences in tuning between decades.\")\n",
    "    \n",
    "    return tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6568c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tuning variations by decade\n",
    "tuning_by_genre_df = analyze_tuning_impact_by_genre()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d9f17",
   "metadata": {},
   "source": [
    "# 7. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8d5e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chord_complexity_analysis(chord_accuracies, label_encoder):\n",
    "    \"\"\"Analyze how chord complexity affects recognition accuracy\"\"\"\n",
    "    # Define chord complexity categories\n",
    "    def get_complexity(chord):\n",
    "        if chord == 'N':  # No chord\n",
    "            return 'No Chord'\n",
    "        elif ':' not in chord:  # Unknown format\n",
    "            return 'Other'\n",
    "        \n",
    "        # Basic classification\n",
    "        root, quality = chord.split(':', 1)\n",
    "        \n",
    "        if quality in ['maj', '']:\n",
    "            return 'Major'\n",
    "        elif quality == 'min':\n",
    "            return 'Minor'\n",
    "        elif quality in ['7', 'maj7', 'min7']:\n",
    "            return 'Seventh'\n",
    "        elif '/' in quality:  # Slash chords (e.g., C:maj/3)\n",
    "            return 'Slash'\n",
    "        elif any(ext in quality for ext in ['9', '11', '13']):\n",
    "            return 'Extended'\n",
    "        elif any(mod in quality for ext in ['dim', 'aug', 'sus']):\n",
    "            return 'Altered'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Group chords by complexity\n",
    "    complexity_groups = {}\n",
    "    \n",
    "    for chord, (acc, count) in chord_accuracies.items():\n",
    "        complexity = get_complexity(chord)\n",
    "        if complexity not in complexity_groups:\n",
    "            complexity_groups[complexity] = []\n",
    "        complexity_groups[complexity].append((chord, acc, count))\n",
    "    \n",
    "    # Calculate average accuracy per complexity group\n",
    "    avg_accuracies = {}\n",
    "    for complexity, chords in complexity_groups.items():\n",
    "        if chords:\n",
    "            total_samples = sum(count for _, _, count in chords)\n",
    "            weighted_acc = sum(acc * count for _, acc, count in chords) / total_samples\n",
    "            avg_accuracies[complexity] = (weighted_acc, total_samples, len(chords))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    complexities = []\n",
    "    accs = []\n",
    "    counts = []\n",
    "    num_chords = []\n",
    "    \n",
    "    for complexity, (acc, count, n_chords) in sorted(avg_accuracies.items()):\n",
    "        complexities.append(complexity)\n",
    "        accs.append(acc)\n",
    "        counts.append(count)\n",
    "        num_chords.append(n_chords)\n",
    "    \n",
    "    bars = plt.bar(complexities, accs, color='skyblue')\n",
    "    \n",
    "    # Add count and number of chords as text\n",
    "    for i, (bar, count, n_chords) in enumerate(zip(bars, counts, num_chords)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{count} samples\\n{n_chords} chords',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel('Chord Complexity')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title('Recognition Accuracy by Chord Complexity')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6324ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chord complexity\n",
    "complexity_results = chord_complexity_analysis(hybrid_chord_accuracies, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final accuracy comparison\n",
    "model_accuracies = {\n",
    "    'CNN': cnn_test_results[1],\n",
    "    'LSTM': lstm_test_results[1],\n",
    "    'Hybrid CNN-LSTM': hybrid_test_results[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d59f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_accuracies.keys(), model_accuracies.values(), color=['cornflowerblue', 'lightseagreen', 'coral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceca5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add accuracy values on top of bars\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.4f}',\n",
    "            ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(0, max(model_accuracies.values()) + 0.1)\n",
    "plt.xlabel('Model Architecture')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2308eb2",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242adb00",
   "metadata": {},
   "source": [
    "In this project, we developed deep learning models for automatic chord recognition using the McGill Billboard dataset. Our approach addressed key challenges including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12f759",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**: We implemented a custom data generator to handle the large dataset without running into memory limitations, avoiding the need for one-hot encoding of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88862a",
   "metadata": {},
   "source": [
    "2. **Temporal Context**: We incorporated sequence modeling to capture the temporal dependencies in music by using windows of consecutive frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323675",
   "metadata": {},
   "source": [
    "3. **Model Architecture Comparison**: We compared three different architectures (CNN, LSTM, and hybrid CNN-LSTM) to understand which approach works best for chord recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e1ac8",
   "metadata": {},
   "source": [
    "4. **Chord Complexity Analysis**: We analyzed how recognition accuracy varies with chord complexity, providing insights into the strengths and limitations of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8a669",
   "metadata": {},
   "source": [
    "## Key Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0a7bd",
   "metadata": {},
   "source": [
    "- The hybrid CNN-LSTM model achieved the best overall performance, likely due to its ability to capture both local patterns in chroma features and temporal dependencies.\n",
    "- Simple chords (major and minor) were recognized more accurately than complex chords (extended, altered, and slash chords).\n",
    "- Using context windows significantly"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
